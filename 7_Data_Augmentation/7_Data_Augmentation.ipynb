{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_Data_Augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYGsZBvtvNUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60e8a43-3a21-423f-8b18-5311cf989b34"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "\n",
        "zip_data = tf.keras.utils.get_file(\n",
        "    origin = dataset_url,\n",
        "    fname = 'flower_photos.tgz',\n",
        "    extract = True\n",
        ")\n",
        "\n",
        "base_directory = os.path.join(os.path.dirname(zip_data), 'flower_photos')\n",
        "\n",
        "data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale = 1.0 / 255.0,\n",
        "    validation_split = 0.2\n",
        ") \n",
        "\n",
        "batchsize = 64\n",
        "\n",
        "training_data = data_generator.flow_from_directory(\n",
        "    base_directory,\n",
        "    target_size = (224, 224),\n",
        "    batch_size = batchsize,\n",
        "    subset = 'training'\n",
        ")\n",
        "\n",
        "testing_data = data_generator.flow_from_directory(\n",
        "    base_directory,\n",
        "    target_size = (224, 224),\n",
        "    batch_size = batchsize,\n",
        "    subset = 'validation'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "228818944/228813984 [==============================] - 2s 0us/step\n",
            "Found 2939 images belonging to 5 classes.\n",
            "Found 731 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4lvYH644QZ1"
      },
      "source": [
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu',\n",
        "         input_shape = (224, 224, 3)\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Flatten(),\n",
        "     tf.keras.layers.Dense(\n",
        "         512,\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.Dense(\n",
        "         5,\n",
        "         activation = 'softmax'\n",
        "     )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgJAZhzw5ZYr",
        "outputId": "e5150629-ddc1-4a0c-92a5-0654ca7c7079"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 222, 222, 64)      1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 111, 111, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 109, 109, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 52, 52, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 43264)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               22151680  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 22,229,893\n",
            "Trainable params: 22,229,893\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCou8NAo5cOF"
      },
      "source": [
        "model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = 'sgd',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTNX-uqD6JBw"
      },
      "source": [
        "training_steps = training_data.samples // batchsize\n",
        "testing_steps = testing_data.samples // batchsize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqJi_ROock7o",
        "outputId": "7b1e00dd-2f88-41da-c2aa-e0f9d802f9b2"
      },
      "source": [
        "testing_steps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaw1MX_a5t6V",
        "outputId": "2a2084d3-9c1b-4169-dc65-fecc780ccc4b"
      },
      "source": [
        "model.fit(\n",
        "    training_data,\n",
        "    validation_data = testing_data,\n",
        "    epochs = 10,\n",
        "    steps_per_epoch = training_steps,\n",
        "    validation_steps = testing_steps,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "45/45 [==============================] - 13s 279ms/step - loss: 0.9365 - accuracy: 0.6393 - val_loss: 1.0773 - val_accuracy: 0.5696\n",
            "Epoch 2/10\n",
            "45/45 [==============================] - 12s 275ms/step - loss: 0.9244 - accuracy: 0.6410 - val_loss: 1.0442 - val_accuracy: 0.5767\n",
            "Epoch 3/10\n",
            "45/45 [==============================] - 12s 274ms/step - loss: 0.8746 - accuracy: 0.6671 - val_loss: 1.0675 - val_accuracy: 0.5753\n",
            "Epoch 4/10\n",
            "45/45 [==============================] - 12s 274ms/step - loss: 0.8549 - accuracy: 0.6734 - val_loss: 1.0365 - val_accuracy: 0.5952\n",
            "Epoch 5/10\n",
            "45/45 [==============================] - 12s 273ms/step - loss: 0.8344 - accuracy: 0.6814 - val_loss: 1.0343 - val_accuracy: 0.5838\n",
            "Epoch 6/10\n",
            "45/45 [==============================] - 12s 273ms/step - loss: 0.7946 - accuracy: 0.7002 - val_loss: 1.0706 - val_accuracy: 0.5810\n",
            "Epoch 7/10\n",
            "45/45 [==============================] - 12s 275ms/step - loss: 0.7814 - accuracy: 0.7071 - val_loss: 1.1743 - val_accuracy: 0.5568\n",
            "Epoch 8/10\n",
            "45/45 [==============================] - 13s 279ms/step - loss: 0.7556 - accuracy: 0.7238 - val_loss: 1.0288 - val_accuracy: 0.5952\n",
            "Epoch 9/10\n",
            "45/45 [==============================] - 13s 281ms/step - loss: 0.7577 - accuracy: 0.7096 - val_loss: 1.0752 - val_accuracy: 0.5909\n",
            "Epoch 10/10\n",
            "45/45 [==============================] - 13s 276ms/step - loss: 0.6834 - accuracy: 0.7464 - val_loss: 1.0405 - val_accuracy: 0.6094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f14720b4190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpzwptihZnqv"
      },
      "source": [
        "![title](https://miro.medium.com/max/1660/1*9hPX9pAO3jqLrzt0IE3JzA.png) \\\n",
        "Source ~ https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTWRjyFxaaOc"
      },
      "source": [
        "**Basic Training and Testing Methodology**\n",
        "![title](https://imgix.lifehacker.com.au/content/uploads/sites/4/2015/03/IMG_20150319_112511.jpg) \\\n",
        "Source ~ https://www.lifehacker.com.au/2015/03/the-basic-recipe-for-machine-learning-explained-in-a-single-powerpoint-slide/ \\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tjtssHma9p8",
        "outputId": "cf74df0e-2133-4417-b14a-67ccd8c9364c"
      },
      "source": [
        "model.fit(\n",
        "    training_data,\n",
        "    validation_data = testing_data,\n",
        "    epochs = 25,\n",
        "    steps_per_epoch = training_steps,\n",
        "    validation_steps = testing_steps,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "45/45 [==============================] - 13s 283ms/step - loss: 0.6787 - accuracy: 0.7513 - val_loss: 1.0207 - val_accuracy: 0.5909\n",
            "Epoch 2/25\n",
            "45/45 [==============================] - 13s 280ms/step - loss: 0.6450 - accuracy: 0.7635 - val_loss: 0.9944 - val_accuracy: 0.6264\n",
            "Epoch 3/25\n",
            "45/45 [==============================] - 13s 278ms/step - loss: 0.6329 - accuracy: 0.7750 - val_loss: 0.9942 - val_accuracy: 0.6378\n",
            "Epoch 4/25\n",
            "45/45 [==============================] - 13s 278ms/step - loss: 0.6127 - accuracy: 0.7774 - val_loss: 1.3853 - val_accuracy: 0.5668\n",
            "Epoch 5/25\n",
            "45/45 [==============================] - 13s 278ms/step - loss: 0.5859 - accuracy: 0.7903 - val_loss: 0.9834 - val_accuracy: 0.6307\n",
            "Epoch 6/25\n",
            "45/45 [==============================] - 13s 279ms/step - loss: 0.5757 - accuracy: 0.8007 - val_loss: 1.0096 - val_accuracy: 0.6122\n",
            "Epoch 7/25\n",
            "45/45 [==============================] - 12s 277ms/step - loss: 0.5270 - accuracy: 0.8083 - val_loss: 0.9817 - val_accuracy: 0.6122\n",
            "Epoch 8/25\n",
            "45/45 [==============================] - 13s 281ms/step - loss: 0.5071 - accuracy: 0.8243 - val_loss: 0.9897 - val_accuracy: 0.6307\n",
            "Epoch 9/25\n",
            "45/45 [==============================] - 13s 278ms/step - loss: 0.4501 - accuracy: 0.8428 - val_loss: 1.0392 - val_accuracy: 0.6023\n",
            "Epoch 10/25\n",
            "45/45 [==============================] - 13s 280ms/step - loss: 0.4601 - accuracy: 0.8497 - val_loss: 1.0749 - val_accuracy: 0.6179\n",
            "Epoch 11/25\n",
            "45/45 [==============================] - 13s 280ms/step - loss: 0.4409 - accuracy: 0.8470 - val_loss: 1.1342 - val_accuracy: 0.5909\n",
            "Epoch 12/25\n",
            "45/45 [==============================] - 12s 276ms/step - loss: 0.3712 - accuracy: 0.8870 - val_loss: 1.0347 - val_accuracy: 0.6278\n",
            "Epoch 13/25\n",
            "45/45 [==============================] - 13s 280ms/step - loss: 0.3608 - accuracy: 0.8873 - val_loss: 1.1203 - val_accuracy: 0.6278\n",
            "Epoch 14/25\n",
            "45/45 [==============================] - 13s 280ms/step - loss: 0.3133 - accuracy: 0.9023 - val_loss: 1.1415 - val_accuracy: 0.6136\n",
            "Epoch 15/25\n",
            "45/45 [==============================] - 13s 281ms/step - loss: 0.3497 - accuracy: 0.8824 - val_loss: 1.1437 - val_accuracy: 0.6037\n",
            "Epoch 16/25\n",
            "45/45 [==============================] - 13s 280ms/step - loss: 0.3070 - accuracy: 0.9120 - val_loss: 1.3121 - val_accuracy: 0.5682\n",
            "Epoch 17/25\n",
            "45/45 [==============================] - 13s 278ms/step - loss: 0.2449 - accuracy: 0.9259 - val_loss: 1.2386 - val_accuracy: 0.6065\n",
            "Epoch 18/25\n",
            "45/45 [==============================] - 13s 281ms/step - loss: 0.2210 - accuracy: 0.9290 - val_loss: 1.2054 - val_accuracy: 0.6065\n",
            "Epoch 19/25\n",
            "45/45 [==============================] - 13s 279ms/step - loss: 0.1748 - accuracy: 0.9586 - val_loss: 1.2560 - val_accuracy: 0.6207\n",
            "Epoch 20/25\n",
            "45/45 [==============================] - 13s 278ms/step - loss: 0.1996 - accuracy: 0.9468 - val_loss: 1.3462 - val_accuracy: 0.5895\n",
            "Epoch 21/25\n",
            "45/45 [==============================] - 12s 275ms/step - loss: 0.1574 - accuracy: 0.9663 - val_loss: 1.2121 - val_accuracy: 0.6250\n",
            "Epoch 22/25\n",
            "45/45 [==============================] - 12s 276ms/step - loss: 0.0936 - accuracy: 0.9833 - val_loss: 1.4260 - val_accuracy: 0.6037\n",
            "Epoch 23/25\n",
            "45/45 [==============================] - 12s 276ms/step - loss: 0.2883 - accuracy: 0.9231 - val_loss: 1.3074 - val_accuracy: 0.6165\n",
            "Epoch 24/25\n",
            "45/45 [==============================] - 13s 279ms/step - loss: 0.1003 - accuracy: 0.9812 - val_loss: 1.3489 - val_accuracy: 0.6250\n",
            "Epoch 25/25\n",
            "45/45 [==============================] - 13s 278ms/step - loss: 0.1470 - accuracy: 0.9701 - val_loss: 1.2560 - val_accuracy: 0.6278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1472252890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMuqa8xMKR3V"
      },
      "source": [
        "**Dropout** ~ It is a type of Stochastic Regualization Method used to tackle Overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Ztw0ZQKxtS"
      },
      "source": [
        "The idea was proposed in paper **Improving neural networks by preventing\n",
        "co-adaptation of feature detectors** by Hinton,et.al. \\\n",
        "Link for the paper ~ https://arxiv.org/pdf/1207.0580.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sUZwYrVIBMv"
      },
      "source": [
        "![title](https://miro.medium.com/max/700/1*iWQzxhVlvadk6VAJjsgXgg.png)<br>\n",
        "Source ~ https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxAaKoYY6pQe"
      },
      "source": [
        "regularized_model = tf.keras.models.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu',\n",
        "         input_shape = (224, 224, 3)\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Dropout(0.5),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Flatten(),\n",
        "     tf.keras.layers.Dense(\n",
        "         512,\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.Dropout(0.5),\n",
        "     tf.keras.layers.Dense(\n",
        "         5,\n",
        "         activation = 'softmax'\n",
        "     )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaJ1oqSSihMB",
        "outputId": "0c09f39e-5bb3-441e-9c24-9b39be7bf18e"
      },
      "source": [
        "regularized_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 222, 222, 64)      1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 111, 111, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 109, 109, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 52, 52, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 43264)             0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               22151680  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 22,229,893\n",
            "Trainable params: 22,229,893\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Cq6JpR9GZq"
      },
      "source": [
        "regularized_model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = 'sgd',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_I3m9iu9LeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378b526f-1307-4dd1-dd90-c6919c31fa45"
      },
      "source": [
        "regularized_model.fit(\n",
        "    training_data,\n",
        "    validation_data = testing_data,\n",
        "    epochs = 25,\n",
        "    steps_per_epoch = training_steps,\n",
        "    validation_steps = testing_steps,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "45/45 [==============================] - 13s 276ms/step - loss: 1.5610 - accuracy: 0.2883 - val_loss: 1.5483 - val_accuracy: 0.3082\n",
            "Epoch 2/25\n",
            "45/45 [==============================] - 12s 269ms/step - loss: 1.4483 - accuracy: 0.3770 - val_loss: 1.4350 - val_accuracy: 0.3679\n",
            "Epoch 3/25\n",
            "45/45 [==============================] - 12s 269ms/step - loss: 1.3214 - accuracy: 0.4403 - val_loss: 1.3785 - val_accuracy: 0.3807\n",
            "Epoch 4/25\n",
            "45/45 [==============================] - 12s 271ms/step - loss: 1.2653 - accuracy: 0.4584 - val_loss: 1.3825 - val_accuracy: 0.4162\n",
            "Epoch 5/25\n",
            "45/45 [==============================] - 12s 270ms/step - loss: 1.1861 - accuracy: 0.4998 - val_loss: 1.2586 - val_accuracy: 0.4673\n",
            "Epoch 6/25\n",
            "45/45 [==============================] - 12s 270ms/step - loss: 1.1441 - accuracy: 0.5287 - val_loss: 1.1737 - val_accuracy: 0.5128\n",
            "Epoch 7/25\n",
            "45/45 [==============================] - 12s 271ms/step - loss: 1.1274 - accuracy: 0.5357 - val_loss: 1.1755 - val_accuracy: 0.5199\n",
            "Epoch 8/25\n",
            "45/45 [==============================] - 12s 274ms/step - loss: 1.0964 - accuracy: 0.5562 - val_loss: 1.1721 - val_accuracy: 0.5142\n",
            "Epoch 9/25\n",
            "45/45 [==============================] - 12s 274ms/step - loss: 1.0669 - accuracy: 0.5565 - val_loss: 1.1668 - val_accuracy: 0.5028\n",
            "Epoch 10/25\n",
            "45/45 [==============================] - 12s 274ms/step - loss: 1.0410 - accuracy: 0.5805 - val_loss: 1.1433 - val_accuracy: 0.5270\n",
            "Epoch 11/25\n",
            "45/45 [==============================] - 12s 275ms/step - loss: 1.0482 - accuracy: 0.5802 - val_loss: 1.1406 - val_accuracy: 0.5483\n",
            "Epoch 12/25\n",
            "45/45 [==============================] - 12s 274ms/step - loss: 1.0036 - accuracy: 0.6108 - val_loss: 1.1023 - val_accuracy: 0.5526\n",
            "Epoch 13/25\n",
            "45/45 [==============================] - 12s 273ms/step - loss: 0.9979 - accuracy: 0.6125 - val_loss: 1.0811 - val_accuracy: 0.5653\n",
            "Epoch 14/25\n",
            "45/45 [==============================] - 12s 275ms/step - loss: 0.9909 - accuracy: 0.6077 - val_loss: 1.1157 - val_accuracy: 0.5455\n",
            "Epoch 15/25\n",
            "45/45 [==============================] - 12s 273ms/step - loss: 0.9613 - accuracy: 0.6261 - val_loss: 1.1109 - val_accuracy: 0.5312\n",
            "Epoch 16/25\n",
            "45/45 [==============================] - 12s 275ms/step - loss: 0.9568 - accuracy: 0.6195 - val_loss: 1.0801 - val_accuracy: 0.5540\n",
            "Epoch 17/25\n",
            "45/45 [==============================] - 12s 273ms/step - loss: 0.9458 - accuracy: 0.6403 - val_loss: 1.0952 - val_accuracy: 0.5540\n",
            "Epoch 18/25\n",
            "45/45 [==============================] - 12s 275ms/step - loss: 0.9135 - accuracy: 0.6369 - val_loss: 1.0960 - val_accuracy: 0.5497\n",
            "Epoch 19/25\n",
            "45/45 [==============================] - 13s 278ms/step - loss: 0.9104 - accuracy: 0.6529 - val_loss: 1.0391 - val_accuracy: 0.5795\n",
            "Epoch 20/25\n",
            "45/45 [==============================] - 12s 273ms/step - loss: 0.8990 - accuracy: 0.6574 - val_loss: 1.0447 - val_accuracy: 0.5810\n",
            "Epoch 21/25\n",
            "45/45 [==============================] - 12s 273ms/step - loss: 0.8773 - accuracy: 0.6664 - val_loss: 1.0504 - val_accuracy: 0.5568\n",
            "Epoch 22/25\n",
            "45/45 [==============================] - 13s 277ms/step - loss: 0.8727 - accuracy: 0.6699 - val_loss: 1.1004 - val_accuracy: 0.5440\n",
            "Epoch 23/25\n",
            "45/45 [==============================] - 12s 276ms/step - loss: 0.8350 - accuracy: 0.6810 - val_loss: 1.0651 - val_accuracy: 0.5682\n",
            "Epoch 24/25\n",
            "45/45 [==============================] - 12s 273ms/step - loss: 0.8224 - accuracy: 0.6842 - val_loss: 1.0585 - val_accuracy: 0.5938\n",
            "Epoch 25/25\n",
            "45/45 [==============================] - 13s 278ms/step - loss: 0.7985 - accuracy: 0.6922 - val_loss: 0.9857 - val_accuracy: 0.6236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f11ec1ccdd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gI5nlFxMK5a"
      },
      "source": [
        "**Data Augmentation** \\\n",
        "Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks. \\\n",
        "Source ~ http://bair.berkeley.edu/blog/2019/06/07/data_aug/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-hE4uqTL3PK"
      },
      "source": [
        "![](https://d3i71xaburhd42.cloudfront.net/27c495019bae2d7ab5b607e11a47a39cd9f1c519/1-Figure1-1.png) \\\n",
        "Source ~ https://arxiv.org/pdf/1708.06020.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv_8rgkvMkNU"
      },
      "source": [
        "![title](https://www.pyimagesearch.com/wp-content/uploads/2019/07/keras_data_augmentation_poll.png) \\\n",
        "Sources ~ https://twitter.com/PyImageSearch/status/1142765698575413248 and https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbnvz6EENckd"
      },
      "source": [
        "![](https://miro.medium.com/max/1210/0*Utma-dS47hSoQ6Zt) \\\n",
        "Source ~ https://towardsdatascience.com/machinex-image-data-augmentation-using-keras-b459ef87cd22\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbHBdFxINkYY"
      },
      "source": [
        "Different types of Data Augmentations : \\\n",
        "![](https://lh5.googleusercontent.com/0gbai1VCDcr6yzMGS4tgVIh96PxZutIr0jnJezdVG5kXxsEDfU9yrqHzKrnARe3i7uSN2CdC3ZCpJytrsOnQOmH4n2Q4o6Z3iNYmM5OZtcR194yRtIXMDP1YeVa9t62oh9o6TUsx-J6BfC51Iw) \\\n",
        "Source ~ https://www.mygreatlearning.com/blog/understanding-data-augmentation/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1o-6s_qi0ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef20507-8c1e-4a4f-b1e0-4ed9e2f29c83"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "\n",
        "zip_data = tf.keras.utils.get_file(\n",
        "    origin = dataset_url,\n",
        "    fname = 'flower_photos.tgz',\n",
        "    extract = True\n",
        ")\n",
        "\n",
        "base_directory = os.path.join(os.path.dirname(zip_data), 'flower_photos')\n",
        "\n",
        "data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale = 1.0 / 255.0,\n",
        "    validation_split = 0.2,\n",
        "    rotation_range = 20,\n",
        "\t\tzoom_range = 0.15,\n",
        "\t\twidth_shift_range = 0.2,\n",
        "\t\theight_shift_range = 0.2,\n",
        "\t\tshear_range = 0.15,\n",
        "\t\thorizontal_flip = True,\n",
        "\t\tfill_mode = \"nearest\"\n",
        ") \n",
        "\n",
        "batchsize = 64\n",
        "\n",
        "training_data = data_generator.flow_from_directory(\n",
        "    base_directory,\n",
        "    target_size = (224, 224),\n",
        "    batch_size = batchsize,\n",
        "    subset = 'training'\n",
        ")\n",
        "\n",
        "testing_data = data_generator.flow_from_directory(\n",
        "    base_directory,\n",
        "    target_size = (224, 224),\n",
        "    batch_size = batchsize,\n",
        "    subset = 'validation'\n",
        ")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "228818944/228813984 [==============================] - 5s 0us/step\n",
            "Found 2939 images belonging to 5 classes.\n",
            "Found 731 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH44tFhQz4aF"
      },
      "source": [
        "augmented_model = tf.keras.models.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu',\n",
        "         input_shape = (224, 224, 3)\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Dropout(0.5),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Flatten(),\n",
        "     tf.keras.layers.Dense(\n",
        "         512,\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.Dropout(0.5),\n",
        "     tf.keras.layers.Dense(\n",
        "         5,\n",
        "         activation = 'softmax'\n",
        "     )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqRks2QR0CM4"
      },
      "source": [
        "augmented_model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = 'sgd',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96dumsb50d-k"
      },
      "source": [
        "training_steps = training_data.samples // batchsize\n",
        "testing_steps = testing_data.samples // batchsize"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byqp1AsI0J68",
        "outputId": "56532a83-bcbc-45d5-f7d2-d39e6c625dfd"
      },
      "source": [
        "augmented_model.fit(\n",
        "    training_data,\n",
        "    validation_data = testing_data,\n",
        "    epochs = 25,\n",
        "    steps_per_epoch = training_steps,\n",
        "    validation_steps = testing_steps,\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "45/45 [==============================] - 87s 1s/step - loss: 1.5798 - accuracy: 0.2824 - val_loss: 1.5502 - val_accuracy: 0.3523\n",
            "Epoch 2/25\n",
            "45/45 [==============================] - 54s 1s/step - loss: 1.4792 - accuracy: 0.3520 - val_loss: 1.4530 - val_accuracy: 0.3466\n",
            "Epoch 3/25\n",
            "45/45 [==============================] - 54s 1s/step - loss: 1.3948 - accuracy: 0.3784 - val_loss: 1.3847 - val_accuracy: 0.3892\n",
            "Epoch 4/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.2954 - accuracy: 0.4184 - val_loss: 1.2921 - val_accuracy: 0.4276\n",
            "Epoch 5/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.2461 - accuracy: 0.4515 - val_loss: 1.2582 - val_accuracy: 0.4531\n",
            "Epoch 6/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.2068 - accuracy: 0.4657 - val_loss: 1.2601 - val_accuracy: 0.4659\n",
            "Epoch 7/25\n",
            "45/45 [==============================] - 54s 1s/step - loss: 1.1805 - accuracy: 0.4915 - val_loss: 1.2512 - val_accuracy: 0.4474\n",
            "Epoch 8/25\n",
            "45/45 [==============================] - 54s 1s/step - loss: 1.1722 - accuracy: 0.4904 - val_loss: 1.2077 - val_accuracy: 0.5057\n",
            "Epoch 9/25\n",
            "45/45 [==============================] - 54s 1s/step - loss: 1.1519 - accuracy: 0.5141 - val_loss: 1.2201 - val_accuracy: 0.4489\n",
            "Epoch 10/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.1350 - accuracy: 0.5162 - val_loss: 1.1725 - val_accuracy: 0.4915\n",
            "Epoch 11/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.1323 - accuracy: 0.5256 - val_loss: 1.1903 - val_accuracy: 0.4744\n",
            "Epoch 12/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.1200 - accuracy: 0.5322 - val_loss: 1.1592 - val_accuracy: 0.5043\n",
            "Epoch 13/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.1091 - accuracy: 0.5395 - val_loss: 1.1399 - val_accuracy: 0.5284\n",
            "Epoch 14/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0994 - accuracy: 0.5593 - val_loss: 1.1507 - val_accuracy: 0.5128\n",
            "Epoch 15/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.0949 - accuracy: 0.5617 - val_loss: 1.1278 - val_accuracy: 0.5185\n",
            "Epoch 16/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.0861 - accuracy: 0.5597 - val_loss: 1.1937 - val_accuracy: 0.4929\n",
            "Epoch 17/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.0799 - accuracy: 0.5597 - val_loss: 1.1501 - val_accuracy: 0.4957\n",
            "Epoch 18/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0562 - accuracy: 0.5781 - val_loss: 1.2029 - val_accuracy: 0.4830\n",
            "Epoch 19/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0662 - accuracy: 0.5729 - val_loss: 1.1252 - val_accuracy: 0.5028\n",
            "Epoch 20/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0498 - accuracy: 0.5816 - val_loss: 1.1163 - val_accuracy: 0.5156\n",
            "Epoch 21/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0556 - accuracy: 0.5809 - val_loss: 1.1180 - val_accuracy: 0.5469\n",
            "Epoch 22/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0262 - accuracy: 0.5743 - val_loss: 1.0997 - val_accuracy: 0.5497\n",
            "Epoch 23/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0286 - accuracy: 0.5878 - val_loss: 1.1042 - val_accuracy: 0.5256\n",
            "Epoch 24/25\n",
            "45/45 [==============================] - 51s 1s/step - loss: 1.0315 - accuracy: 0.5784 - val_loss: 1.0562 - val_accuracy: 0.5895\n",
            "Epoch 25/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0293 - accuracy: 0.5857 - val_loss: 1.0794 - val_accuracy: 0.5540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa527eedc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ2GmEfq0e8t",
        "outputId": "c0ddceac-8340-4702-e7c5-b95f08d4bc2a"
      },
      "source": [
        "augmented_model = tf.keras.models.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu',\n",
        "         input_shape = (224, 224, 3)\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Flatten(),\n",
        "     tf.keras.layers.Dense(\n",
        "         512,\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.Dense(\n",
        "         5,\n",
        "         activation = 'softmax'\n",
        "     )\n",
        "    ]\n",
        ")\n",
        "\n",
        "augmented_model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = 'sgd',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "augmented_model.fit(\n",
        "    training_data,\n",
        "    validation_data = testing_data,\n",
        "    epochs = 25,\n",
        "    steps_per_epoch = training_steps,\n",
        "    validation_steps = testing_steps,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "45/45 [==============================] - 55s 1s/step - loss: 1.5782 - accuracy: 0.2897 - val_loss: 1.5447 - val_accuracy: 0.3068\n",
            "Epoch 2/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.5029 - accuracy: 0.3315 - val_loss: 1.4996 - val_accuracy: 0.3253\n",
            "Epoch 3/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.4256 - accuracy: 0.3732 - val_loss: 1.3624 - val_accuracy: 0.4020\n",
            "Epoch 4/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.3435 - accuracy: 0.4010 - val_loss: 1.2996 - val_accuracy: 0.4290\n",
            "Epoch 5/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.2673 - accuracy: 0.4470 - val_loss: 1.2326 - val_accuracy: 0.4730\n",
            "Epoch 6/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.2113 - accuracy: 0.4911 - val_loss: 1.4636 - val_accuracy: 0.3537\n",
            "Epoch 7/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.1654 - accuracy: 0.5019 - val_loss: 1.1554 - val_accuracy: 0.5199\n",
            "Epoch 8/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.1147 - accuracy: 0.5430 - val_loss: 1.1659 - val_accuracy: 0.5213\n",
            "Epoch 9/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.1125 - accuracy: 0.5409 - val_loss: 1.1570 - val_accuracy: 0.5469\n",
            "Epoch 10/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0922 - accuracy: 0.5638 - val_loss: 1.0893 - val_accuracy: 0.5682\n",
            "Epoch 11/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.0811 - accuracy: 0.5652 - val_loss: 1.1320 - val_accuracy: 0.5426\n",
            "Epoch 12/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.0565 - accuracy: 0.5882 - val_loss: 1.1016 - val_accuracy: 0.5781\n",
            "Epoch 13/25\n",
            "45/45 [==============================] - 54s 1s/step - loss: 1.0603 - accuracy: 0.5784 - val_loss: 1.1776 - val_accuracy: 0.5625\n",
            "Epoch 14/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.0441 - accuracy: 0.5899 - val_loss: 1.0781 - val_accuracy: 0.5739\n",
            "Epoch 15/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.0230 - accuracy: 0.6052 - val_loss: 1.0651 - val_accuracy: 0.6065\n",
            "Epoch 16/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0097 - accuracy: 0.5955 - val_loss: 1.0538 - val_accuracy: 0.5795\n",
            "Epoch 17/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 1.0012 - accuracy: 0.6118 - val_loss: 1.0473 - val_accuracy: 0.6037\n",
            "Epoch 18/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 1.0089 - accuracy: 0.5927 - val_loss: 1.0247 - val_accuracy: 0.6136\n",
            "Epoch 19/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 0.9719 - accuracy: 0.6289 - val_loss: 1.0351 - val_accuracy: 0.6094\n",
            "Epoch 20/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 0.9901 - accuracy: 0.6205 - val_loss: 1.0226 - val_accuracy: 0.6065\n",
            "Epoch 21/25\n",
            "45/45 [==============================] - 52s 1s/step - loss: 0.9689 - accuracy: 0.6285 - val_loss: 1.0191 - val_accuracy: 0.6108\n",
            "Epoch 22/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 0.9574 - accuracy: 0.6397 - val_loss: 1.0224 - val_accuracy: 0.5980\n",
            "Epoch 23/25\n",
            "45/45 [==============================] - 54s 1s/step - loss: 0.9625 - accuracy: 0.6271 - val_loss: 1.0895 - val_accuracy: 0.5810\n",
            "Epoch 24/25\n",
            "45/45 [==============================] - 54s 1s/step - loss: 0.9696 - accuracy: 0.6212 - val_loss: 1.0064 - val_accuracy: 0.6179\n",
            "Epoch 25/25\n",
            "45/45 [==============================] - 53s 1s/step - loss: 0.9225 - accuracy: 0.6358 - val_loss: 0.9888 - val_accuracy: 0.6151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa481dc7ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq60vqrs8zZv",
        "outputId": "d46a2818-24c0-426f-f73e-568fe2e15901"
      },
      "source": [
        "augmented_model = tf.keras.models.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.Conv2D(\n",
        "         128,\n",
        "         (3, 3),\n",
        "         activation = 'relu',\n",
        "         input_shape = (224, 224, 3)\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         128,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Conv2D(\n",
        "         64,\n",
        "         (3, 3),\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "     tf.keras.layers.Flatten(),\n",
        "     tf.keras.layers.Dense(\n",
        "         512,\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.Dense(\n",
        "         256,\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.Dense(\n",
        "         128,\n",
        "         activation = 'relu'\n",
        "     ),\n",
        "     tf.keras.layers.Dense(\n",
        "         5,\n",
        "         activation = 'softmax'\n",
        "     )\n",
        "    ]\n",
        ")\n",
        "\n",
        "augmented_model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = 'sgd',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "augmented_model.fit(\n",
        "    training_data,\n",
        "    validation_data = testing_data,\n",
        "    epochs = 25,\n",
        "    steps_per_epoch = training_steps,\n",
        "    validation_steps = testing_steps,\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "45/45 [==============================] - 71s 1s/step - loss: 1.5937 - accuracy: 0.2953 - val_loss: 1.5822 - val_accuracy: 0.3509\n",
            "Epoch 2/25\n",
            "45/45 [==============================] - 58s 1s/step - loss: 1.5465 - accuracy: 0.3607 - val_loss: 1.5230 - val_accuracy: 0.3764\n",
            "Epoch 3/25\n",
            "45/45 [==============================] - 58s 1s/step - loss: 1.4677 - accuracy: 0.3457 - val_loss: 1.4051 - val_accuracy: 0.3977\n",
            "Epoch 4/25\n",
            "45/45 [==============================] - 59s 1s/step - loss: 1.3705 - accuracy: 0.4059 - val_loss: 1.3396 - val_accuracy: 0.4290\n",
            "Epoch 5/25\n",
            "45/45 [==============================] - 58s 1s/step - loss: 1.2956 - accuracy: 0.4511 - val_loss: 1.2790 - val_accuracy: 0.4446\n",
            "Epoch 6/25\n",
            "45/45 [==============================] - 58s 1s/step - loss: 1.2261 - accuracy: 0.4821 - val_loss: 1.2171 - val_accuracy: 0.4702\n",
            "Epoch 7/25\n",
            "45/45 [==============================] - 58s 1s/step - loss: 1.1722 - accuracy: 0.5103 - val_loss: 1.1726 - val_accuracy: 0.4957\n",
            "Epoch 8/25\n",
            "45/45 [==============================] - 58s 1s/step - loss: 1.1547 - accuracy: 0.5315 - val_loss: 1.2151 - val_accuracy: 0.5469\n",
            "Epoch 9/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 1.1149 - accuracy: 0.5551 - val_loss: 1.1796 - val_accuracy: 0.5256\n",
            "Epoch 10/25\n",
            "45/45 [==============================] - 56s 1s/step - loss: 1.1184 - accuracy: 0.5565 - val_loss: 1.1371 - val_accuracy: 0.5554\n",
            "Epoch 11/25\n",
            "45/45 [==============================] - 56s 1s/step - loss: 1.0732 - accuracy: 0.5652 - val_loss: 1.1422 - val_accuracy: 0.5469\n",
            "Epoch 12/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 1.0671 - accuracy: 0.5788 - val_loss: 1.0941 - val_accuracy: 0.5597\n",
            "Epoch 13/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 1.0451 - accuracy: 0.5805 - val_loss: 1.1208 - val_accuracy: 0.5639\n",
            "Epoch 14/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 1.0537 - accuracy: 0.5934 - val_loss: 1.0969 - val_accuracy: 0.5483\n",
            "Epoch 15/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 1.0279 - accuracy: 0.5910 - val_loss: 1.0825 - val_accuracy: 0.5625\n",
            "Epoch 16/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 1.0193 - accuracy: 0.5965 - val_loss: 1.1513 - val_accuracy: 0.5469\n",
            "Epoch 17/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 1.0140 - accuracy: 0.5927 - val_loss: 1.1104 - val_accuracy: 0.5696\n",
            "Epoch 18/25\n",
            "45/45 [==============================] - 59s 1s/step - loss: 1.0138 - accuracy: 0.5958 - val_loss: 1.0726 - val_accuracy: 0.5810\n",
            "Epoch 19/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 0.9903 - accuracy: 0.6070 - val_loss: 1.0309 - val_accuracy: 0.6165\n",
            "Epoch 20/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 0.9794 - accuracy: 0.6153 - val_loss: 1.1336 - val_accuracy: 0.5483\n",
            "Epoch 21/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 0.9794 - accuracy: 0.6115 - val_loss: 1.0518 - val_accuracy: 0.5923\n",
            "Epoch 22/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 0.9663 - accuracy: 0.6212 - val_loss: 1.0475 - val_accuracy: 0.6065\n",
            "Epoch 23/25\n",
            "45/45 [==============================] - 57s 1s/step - loss: 0.9631 - accuracy: 0.6303 - val_loss: 1.0335 - val_accuracy: 0.5923\n",
            "Epoch 24/25\n",
            "45/45 [==============================] - 58s 1s/step - loss: 0.9521 - accuracy: 0.6313 - val_loss: 0.9530 - val_accuracy: 0.6222\n",
            "Epoch 25/25\n",
            "45/45 [==============================] - 58s 1s/step - loss: 0.9379 - accuracy: 0.6285 - val_loss: 0.9976 - val_accuracy: 0.6037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa481c74a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK68iPqlD8IQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}